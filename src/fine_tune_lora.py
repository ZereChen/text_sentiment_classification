# -*- coding: utf-8 -*-
"""Fine Tuned BERT for Movie Review Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jCrUI1WxOHq06nctLczmMCA94xhfhzJR
"""

import json
import os

import torch
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training
from sklearn.metrics import f1_score
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import (
    get_linear_schedule_with_warmup,
)
import torch.nn as nn

from src.data.preprocess import TextDataset, load_data
from src.utils.log_utils import LoggerManager
from src.utils.model_loader import ModelLoader

logger = LoggerManager.get_logger(__name__)


class LoRATrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else
            "mps" if torch.backends.mps.is_available() else
            "cpu"
        )
        logger.info(f"使用设备: {self.device}")

        # 初始化模型和tokenizer
        bert, tokenizer = ModelLoader.load_pretrained(
            model_name=config['model_name'],
            is_modelscope=True,
            num_labels=config['num_classes'],
            # 数字 ID → 字符串标签（用于模型输出后的解读
            id2label={0: "负面", 1: "正面"},
            # 字符串标签 → 数字 ID（用于输入模型前的预处理）
            label2id={"负面": 0, "正面": 1}
        )
        self.model = bert
        self.tokenizer = tokenizer

        # 配置LoRA
        peft_config = LoraConfig(
            task_type=TaskType.SEQ_CLS,
            inference_mode=False,
            r=config['lora_r'],
            lora_alpha=config['lora_alpha'],
            lora_dropout=config['lora_dropout'],
            target_modules=["query", "key", "value"]  # 针对BERT的注意力层
        )

        # 得到LoRA模型
        self.model = prepare_model_for_kbit_training(self.model)
        # 将 LoRA 注入模型
        self.model = get_peft_model(self.model, peft_config)
        self.model.to(self.device)

        # 打印可训练参数数量
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logger.info(f"可训练参数数量: {trainable_params:,}")

    def prepare_data(self, texts, labels):
        """
        创建数据集，按训练集和验证集8:2比例分
        :param texts:
        :param labels:
        :return:
        """
        dataset = TextDataset(texts, labels, self.tokenizer, self.config['max_length'])
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

        # 创建数据加载器
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=4
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=4
        )

        return train_loader, val_loader

    def train(self, train_loader, val_loader):
        # 优化器设置
        optimizer = AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        # 总步骤数
        total_steps = len(train_loader) * self.config['num_epochs']
        # 学习率预热，num_warmup_steps表示前 int(total_steps * 0.1) 步为预热阶段。
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(total_steps * 0.1),
            num_training_steps=total_steps
        )

        # 训练循环
        best_val_f1 = 0.0
        for epoch in range(self.config['num_epochs']):
            logger.info(f"开始第 {epoch + 1} 轮训练")

            # 训练阶段
            self.model.train()
            total_loss = 0
            train_steps = 0

            for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                # 前向传播
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                criterion = nn.CrossEntropyLoss()
                loss = criterion(outputs.logits, labels)
                # 累计整个 epoch 的总损失
                total_loss += loss.item()
                train_steps += 1

                # 反向传播
                loss.backward()
                # 梯度裁剪防止梯度爆炸，计算所有参数梯度的 L2 范数
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])
                # 当前梯度更新模型参数（反向传播）
                optimizer.step()
                # 更新学习率
                scheduler.step()
                # 清空梯度缓冲区（如果不清零，下一个 batch 的梯度会加到上一个 batch 的梯度上）
                optimizer.zero_grad()
            # 计算平均 loss
            avg_train_loss = total_loss / train_steps
            logger.info(f"第 {epoch + 1} 轮平均训练损失: {avg_train_loss:.4f}")

            # 验证阶段
            val_f1 = self.evaluate(val_loader)
            logger.info(f"第 {epoch + 1} 轮验证集 F1 分数: {val_f1:.4f}")

            # 保存最佳模型
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                self.save_model(f"outputs/lora_best_model")
                logger.info(f"保存新的最佳模型，F1 分数: {val_f1:.4f}")

    def evaluate(self, val_loader):
        self.model.eval()
        # 存储所有预测结果
        all_preds = []
        # 存储所有真实标签
        all_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels']
                # 前向传播进行预测
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                # 取每个样本 logits 中最大值的索引 作为预测结果
                preds = torch.argmax(outputs.logits, dim=1).cpu()
                all_preds.extend(preds.numpy())
                all_labels.extend(labels.numpy())

        return f1_score(all_labels, all_preds, average='weighted')

    def save_model(self, output_dir):
        os.makedirs(output_dir, exist_ok=True)
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        # 保存配置
        config_path = os.path.join(output_dir, 'training_config.json')
        with open(config_path, 'w') as f:
            json.dump(self.config, f, indent=4)


def main():
    # 配置参数
    config = {
        'model_name': 'google-bert/bert-base-chinese',
        'num_classes': 2,
        'max_length': 128,
        'batch_size': 16,
        'num_epochs': 5,
        'learning_rate': 2e-4,
        'weight_decay': 0.01,
        'max_grad_norm': 1.0,
        'lora_r': 8,
        'lora_alpha': 16,
        'lora_dropout': 0.1
    }

    # 加载数据
    logger.info("加载数据...")
    texts, labels = load_data('../data/Data.csv')

    # 初始化训练器
    trainer = LoRATrainer(config)

    # 准备数据
    train_loader, val_loader = trainer.prepare_data(texts, labels)

    # 开始训练
    logger.info("开始训练...")
    trainer.train(train_loader, val_loader)
    logger.info("训练完成！")


if __name__ == "__main__":
    main()
