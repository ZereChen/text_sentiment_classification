2025-06-09 11:45:30,723 - root - INFO - 使用设备: cuda
2025-06-09 11:45:30,724 - root - INFO - 加载数据...
2025-06-09 11:45:30,752 - root - INFO - 开始超参数优化...
2025-06-09 11:45:30,756 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 2.9075857007789992e-05, 'weight_decay': 0.07285830564741734, 'dropout_rate': 0.3960120066248135, 'batch_size': 32, 'max_length': 64, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 11:45:30,757 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 11:45:30,868 - src.utils.model_loader - INFO - 从本地缓存加载ModelScope模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 11:47:21,454 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.9161
2025-06-09 11:47:21,456 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 11:47:21,460 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 11:49:02,609 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.9555
2025-06-09 11:49:02,613 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 11:49:02,616 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 11:50:43,643 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.9807
2025-06-09 11:50:43,647 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.9807, 最佳模型下标(折数): 2, 以下是所有模型信息:
2025-06-09 11:50:43,650 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.9161
2025-06-09 11:50:43,653 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.9555
2025-06-09 11:50:43,656 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.9807
2025-06-09 11:50:43,660 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 1.3799733212718584e-05, 'weight_decay': 0.08388063341978763, 'dropout_rate': 0.36996051769255733, 'batch_size': 16, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 11:50:43,664 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 11:50:43,667 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 11:56:56,028 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.9873
2025-06-09 11:56:56,031 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 11:56:56,035 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:03:08,699 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.9915
2025-06-09 12:03:08,702 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 12:03:08,705 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:09:22,273 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.9945
2025-06-09 12:09:22,277 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.9945, 最佳模型下标(折数): 2, 以下是所有模型信息:
2025-06-09 12:09:22,280 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.9873
2025-06-09 12:09:22,283 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.9915
2025-06-09 12:09:22,289 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.9945
2025-06-09 12:09:22,293 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 1.4202407793766412e-05, 'weight_decay': 0.038285320703228966, 'dropout_rate': 0.483172504565729, 'batch_size': 16, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 12:09:22,297 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 12:09:22,300 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:15:35,459 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.9952
2025-06-09 12:15:35,463 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 12:15:35,466 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:21:48,928 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.9950
2025-06-09 12:21:48,932 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 12:21:48,935 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:28:02,583 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.9975
2025-06-09 12:28:02,587 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.9975, 最佳模型下标(折数): 2, 以下是所有模型信息:
2025-06-09 12:28:02,590 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.9952
2025-06-09 12:28:02,593 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.9950
2025-06-09 12:28:02,596 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.9975
2025-06-09 12:28:02,600 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 4.2431261514620276e-05, 'weight_decay': 0.04061179868048655, 'dropout_rate': 0.3079469552949188, 'batch_size': 8, 'max_length': 64, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 12:28:02,604 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 12:28:02,607 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:30:36,143 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.9612
2025-06-09 12:30:36,148 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 12:30:36,151 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:33:09,389 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.9606
2025-06-09 12:33:09,394 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 12:33:09,397 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:35:42,327 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.9593
2025-06-09 12:35:42,330 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.9612, 最佳模型下标(折数): 0, 以下是所有模型信息:
2025-06-09 12:35:42,334 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.9612
2025-06-09 12:35:42,337 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.9606
2025-06-09 12:35:42,340 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.9593
2025-06-09 12:35:42,344 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 3.8648017946685547e-05, 'weight_decay': 0.02522091662397049, 'dropout_rate': 0.3455968837862207, 'batch_size': 8, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 12:35:42,348 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 12:35:42,351 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:39:34,942 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.9418
2025-06-09 12:39:34,945 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 12:39:34,948 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:43:26,222 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.9149
2025-06-09 12:43:26,226 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 12:43:26,228 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:47:17,583 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.9170
2025-06-09 12:47:17,587 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.9418, 最佳模型下标(折数): 0, 以下是所有模型信息:
2025-06-09 12:47:17,590 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.9418
2025-06-09 12:47:17,593 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.9149
2025-06-09 12:47:17,596 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.9170
2025-06-09 12:47:17,600 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 2.9500998206378228e-05, 'weight_decay': 0.045393782300153666, 'dropout_rate': 0.2687588653959364, 'batch_size': 32, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 12:47:17,604 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 12:47:17,607 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:50:19,538 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.5368
2025-06-09 12:50:19,542 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 12:50:19,545 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:53:21,526 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.5389
2025-06-09 12:53:21,529 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 12:53:21,533 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:56:23,325 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.5335
2025-06-09 12:56:23,328 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.5389, 最佳模型下标(折数): 1, 以下是所有模型信息:
2025-06-09 12:56:23,332 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.5368
2025-06-09 12:56:23,335 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.5389
2025-06-09 12:56:23,338 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.5335
2025-06-09 12:56:23,343 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 1.4623740955856762e-05, 'weight_decay': 0.07499303608003831, 'dropout_rate': 0.18348921620751205, 'batch_size': 16, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 12:56:23,348 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 12:56:23,351 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 12:59:40,816 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.5282
2025-06-09 12:59:40,820 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 12:59:40,823 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:02:58,432 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.5369
2025-06-09 13:02:58,436 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 13:02:58,439 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:06:16,336 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.5335
2025-06-09 13:06:16,340 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.5369, 最佳模型下标(折数): 1, 以下是所有模型信息:
2025-06-09 13:06:16,342 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.5282
2025-06-09 13:06:16,345 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.5369
2025-06-09 13:06:16,348 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.5335
2025-06-09 13:06:16,353 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 4.138342722238713e-05, 'weight_decay': 0.01377116743501255, 'dropout_rate': 0.3906262297770029, 'batch_size': 8, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 13:06:16,356 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 13:06:16,360 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:13:01,354 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.5282
2025-06-09 13:13:01,357 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 13:13:01,361 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:19:46,174 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.5369
2025-06-09 13:19:46,178 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 13:19:46,182 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:26:30,795 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.5335
2025-06-09 13:26:30,798 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.5369, 最佳模型下标(折数): 1, 以下是所有模型信息:
2025-06-09 13:26:30,802 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.5282
2025-06-09 13:26:30,805 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.5369
2025-06-09 13:26:30,808 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.5335
2025-06-09 13:26:30,812 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 3.08629349911997e-05, 'weight_decay': 0.034731687930011045, 'dropout_rate': 0.4468301481617918, 'batch_size': 8, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 13:26:30,816 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 13:26:30,819 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:33:15,324 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.5282
2025-06-09 13:33:15,325 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 13:33:15,328 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:39:59,781 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.5369
2025-06-09 13:39:59,785 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 13:39:59,788 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:46:44,287 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.5335
2025-06-09 13:46:44,289 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.5369, 最佳模型下标(折数): 1, 以下是所有模型信息:
2025-06-09 13:46:44,294 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.5282
2025-06-09 13:46:44,299 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.5369
2025-06-09 13:46:44,305 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.5335
2025-06-09 13:46:44,311 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 2.9950078561695235e-05, 'weight_decay': 0.09021123898649323, 'dropout_rate': 0.1297016801905156, 'batch_size': 16, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 13:46:44,314 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 3 折
2025-06-09 13:46:44,317 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:50:00,891 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.5282
2025-06-09 13:50:00,895 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 3 折
2025-06-09 13:50:00,898 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:53:17,409 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.5369
2025-06-09 13:53:17,412 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 3 折
2025-06-09 13:53:17,417 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 13:56:33,526 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.5335
2025-06-09 13:56:33,529 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.5369, 最佳模型下标(折数): 1, 以下是所有模型信息:
2025-06-09 13:56:33,532 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.5282
2025-06-09 13:56:33,536 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.5369
2025-06-09 13:56:33,539 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.5335
2025-06-09 13:56:33,542 - root - INFO - 获取最佳超参数成功，具体数值为:
2025-06-09 13:56:33,545 - root - INFO - learning_rate: 1.4202407793766412e-05
2025-06-09 13:56:33,548 - root - INFO - weight_decay: 0.038285320703228966
2025-06-09 13:56:33,553 - root - INFO - dropout_rate: 0.483172504565729
2025-06-09 13:56:33,556 - root - INFO - batch_size: 16
2025-06-09 13:56:33,559 - root - INFO - max_length: 256
2025-06-09 13:56:33,562 - root - INFO - 使用最佳超参数进行最终训练...
2025-06-09 13:56:33,571 - root - INFO - 开始新一轮的交叉验证训练, 索引为: 4001692125735695379, 具体参数为: {'learning_rate': 1.4202407793766412e-05, 'weight_decay': 0.038285320703228966, 'dropout_rate': 0.483172504565729, 'batch_size': 16, 'max_length': 256, 'num_epochs': 10, 'n_folds': 5, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2025-06-09 13:56:33,575 - root - INFO - 索引4001692125735695379 开始训练第 0 折, 共 5 折
2025-06-09 13:56:33,578 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 14:19:31,713 - root - INFO - 索引4001692125735695379 完成训练第 0 折，F1分数: 0.5308
2025-06-09 14:19:31,716 - root - INFO - 索引4001692125735695379 开始训练第 1 折, 共 5 折
2025-06-09 14:19:31,721 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 14:42:33,296 - root - INFO - 索引4001692125735695379 完成训练第 1 折，F1分数: 0.5298
2025-06-09 14:42:33,297 - root - INFO - 索引4001692125735695379 开始训练第 2 折, 共 5 折
2025-06-09 14:42:33,302 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 15:05:42,466 - root - INFO - 索引4001692125735695379 完成训练第 2 折，F1分数: 0.5328
2025-06-09 15:05:42,471 - root - INFO - 索引4001692125735695379 开始训练第 3 折, 共 5 折
2025-06-09 15:05:42,473 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 15:28:50,111 - root - INFO - 索引4001692125735695379 完成训练第 3 折，F1分数: 0.5243
2025-06-09 15:28:50,114 - root - INFO - 索引4001692125735695379 开始训练第 4 折, 共 5 折
2025-06-09 15:28:50,116 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2025-06-09 15:51:55,749 - root - INFO - 索引4001692125735695379 完成训练第 4 折，F1分数: 0.5467
2025-06-09 15:51:55,753 - root - INFO - 此轮的交叉验证训练完成, 索引为: 4001692125735695379, 最佳F1分数: 0.5467, 最佳模型下标(折数): 4, 以下是所有模型信息:
2025-06-09 15:51:55,756 - root - INFO - 	 模型下标(折数) 0: F1分数 = 0.5308
2025-06-09 15:51:55,759 - root - INFO - 	 模型下标(折数) 1: F1分数 = 0.5298
2025-06-09 15:51:55,762 - root - INFO - 	 模型下标(折数) 2: F1分数 = 0.5328
2025-06-09 15:51:55,765 - root - INFO - 	 模型下标(折数) 3: F1分数 = 0.5243
2025-06-09 15:51:55,768 - root - INFO - 	 模型下标(折数) 4: F1分数 = 0.5467
2025-06-09 15:51:55,771 - root - INFO - 最终训练结束，最佳验证F1分数: 0.5467, 最佳模型下标(折数): 4, 以下是所有模型信息:
2025-06-09 15:51:55,774 - root - INFO - 	 训练模型下标(折数) 0: F1分数 = 0.5308
2025-06-09 15:51:55,778 - root - INFO - 	 训练模型下标(折数) 1: F1分数 = 0.5298
2025-06-09 15:51:55,782 - root - INFO - 	 训练模型下标(折数) 2: F1分数 = 0.5328
2025-06-09 15:51:55,785 - root - INFO - 	 训练模型下标(折数) 3: F1分数 = 0.5243
2025-06-09 15:51:55,789 - root - INFO - 	 训练模型下标(折数) 4: F1分数 = 0.5467
2025-06-09 15:52:08,000 - root - INFO - 模型保存成功，训练完成！
