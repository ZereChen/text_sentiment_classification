2026-01-04 17:45:06,170 - src.utils.model_loader - INFO - HuggingFace模型缓存目录: /root/.cache/huggingface/hub
2026-01-04 17:45:06,171 - src.utils.model_loader - INFO - ModelScope模型缓存目录: /root/.cache/modelscope/hub/models
2026-01-04 17:45:06,184 - src.utils.model_loader - INFO - 使用设备: cuda
2026-01-04 17:45:06,185 - src.utils.model_loader - INFO - 加载数据...
2026-01-04 17:45:06,316 - src.utils.model_loader - INFO - 开始超参数优化...
2026-01-04 17:45:06,330 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: 4200521160777990757, 具体参数为: {'learning_rate': 2.798502718214016e-05, 'weight_decay': 0.09604286286684563, 'dropout_rate': 0.14052003934040244, 'batch_size': 128, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 17:45:06,344 - src.utils.model_loader - INFO - 索引 4200521160777990757 开始训练第 0 折, 共 3 折
2026-01-04 17:45:06,918 - src.utils.model_loader - INFO - 从本地缓存目录加载ModelScope模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 17:48:20,142 - src.utils.model_loader - INFO - 索引 4200521160777990757 完成训练第 0 折，F1分数: 0.9128
2026-01-04 17:48:20,143 - src.utils.model_loader - INFO - 索引 4200521160777990757 开始训练第 1 折, 共 3 折
2026-01-04 17:48:20,154 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 17:51:10,097 - src.utils.model_loader - INFO - 索引 4200521160777990757 完成训练第 1 折，F1分数: 0.9508
2026-01-04 17:51:10,098 - src.utils.model_loader - INFO - 索引 4200521160777990757 开始训练第 2 折, 共 3 折
2026-01-04 17:51:10,101 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 17:53:59,337 - src.utils.model_loader - INFO - 索引 4200521160777990757 完成训练第 2 折，F1分数: 0.9784
2026-01-04 17:53:59,340 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: 4200521160777990757, 平均F1分数: 0.9473, 最佳F1分数: 0.9784, 最佳模型下标(折数): 2, 以下是所有模型信息:
2026-01-04 17:53:59,343 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9128
2026-01-04 17:53:59,346 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9508
2026-01-04 17:53:59,348 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9784
2026-01-04 17:53:59,358 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: -3159560092909006628, 具体参数为: {'learning_rate': 1.0250521398015947e-05, 'weight_decay': 0.016735253647747424, 'dropout_rate': 0.4952868457320402, 'batch_size': 64, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 17:53:59,361 - src.utils.model_loader - INFO - 索引 -3159560092909006628 开始训练第 0 折, 共 3 折
2026-01-04 17:53:59,366 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 17:59:43,449 - src.utils.model_loader - INFO - 索引 -3159560092909006628 完成训练第 0 折，F1分数: 0.9890
2026-01-04 17:59:43,450 - src.utils.model_loader - INFO - 索引 -3159560092909006628 开始训练第 1 折, 共 3 折
2026-01-04 17:59:43,455 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:05:26,795 - src.utils.model_loader - INFO - 索引 -3159560092909006628 完成训练第 1 折，F1分数: 0.9942
2026-01-04 18:05:26,799 - src.utils.model_loader - INFO - 索引 -3159560092909006628 开始训练第 2 折, 共 3 折
2026-01-04 18:05:26,801 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:11:09,759 - src.utils.model_loader - INFO - 索引 -3159560092909006628 完成训练第 2 折，F1分数: 0.9962
2026-01-04 18:11:09,762 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: -3159560092909006628, 平均F1分数: 0.9932, 最佳F1分数: 0.9962, 最佳模型下标(折数): 2, 以下是所有模型信息:
2026-01-04 18:11:09,765 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9890
2026-01-04 18:11:09,766 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9942
2026-01-04 18:11:09,768 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9962
2026-01-04 18:11:09,771 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: -3639337481344038214, 具体参数为: {'learning_rate': 1.6081966473745243e-05, 'weight_decay': 0.02698200177017849, 'dropout_rate': 0.3067066217763812, 'batch_size': 32, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 18:11:09,776 - src.utils.model_loader - INFO - 索引 -3639337481344038214 开始训练第 0 折, 共 3 折
2026-01-04 18:11:09,778 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:14:16,408 - src.utils.model_loader - INFO - 索引 -3639337481344038214 完成训练第 0 折，F1分数: 0.9942
2026-01-04 18:14:16,411 - src.utils.model_loader - INFO - 索引 -3639337481344038214 开始训练第 1 折, 共 3 折
2026-01-04 18:14:16,413 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:17:23,777 - src.utils.model_loader - INFO - 索引 -3639337481344038214 完成训练第 1 折，F1分数: 0.9952
2026-01-04 18:17:23,781 - src.utils.model_loader - INFO - 索引 -3639337481344038214 开始训练第 2 折, 共 3 折
2026-01-04 18:17:23,783 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:20:30,856 - src.utils.model_loader - INFO - 索引 -3639337481344038214 完成训练第 2 折，F1分数: 0.9965
2026-01-04 18:20:30,859 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: -3639337481344038214, 平均F1分数: 0.9953, 最佳F1分数: 0.9965, 最佳模型下标(折数): 2, 以下是所有模型信息:
2026-01-04 18:20:30,862 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9942
2026-01-04 18:20:30,865 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9952
2026-01-04 18:20:30,867 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9965
2026-01-04 18:20:30,871 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: -5217989591085914, 具体参数为: {'learning_rate': 4.697526591593099e-05, 'weight_decay': 0.027973980388947925, 'dropout_rate': 0.42022493379217296, 'batch_size': 8, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 18:20:30,874 - src.utils.model_loader - INFO - 索引 -5217989591085914 开始训练第 0 折, 共 3 折
2026-01-04 18:20:30,877 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:27:31,513 - src.utils.model_loader - INFO - 索引 -5217989591085914 完成训练第 0 折，F1分数: 0.9232
2026-01-04 18:27:31,516 - src.utils.model_loader - INFO - 索引 -5217989591085914 开始训练第 1 折, 共 3 折
2026-01-04 18:27:31,519 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:34:29,435 - src.utils.model_loader - INFO - 索引 -5217989591085914 完成训练第 1 折，F1分数: 0.8960
2026-01-04 18:34:29,439 - src.utils.model_loader - INFO - 索引 -5217989591085914 开始训练第 2 折, 共 3 折
2026-01-04 18:34:29,442 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:41:27,300 - src.utils.model_loader - INFO - 索引 -5217989591085914 完成训练第 2 折，F1分数: 0.8391
2026-01-04 18:41:27,303 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: -5217989591085914, 平均F1分数: 0.8861, 最佳F1分数: 0.9232, 最佳模型下标(折数): 0, 以下是所有模型信息:
2026-01-04 18:41:27,306 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9232
2026-01-04 18:41:27,307 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.8960
2026-01-04 18:41:27,309 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.8391
2026-01-04 18:41:27,313 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: 7892926386913577697, 具体参数为: {'learning_rate': 1.4683277152960546e-05, 'weight_decay': 0.09880983059128852, 'dropout_rate': 0.41093912042018754, 'batch_size': 8, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 18:41:27,316 - src.utils.model_loader - INFO - 索引 7892926386913577697 开始训练第 0 折, 共 3 折
2026-01-04 18:41:27,319 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:48:26,262 - src.utils.model_loader - INFO - 索引 7892926386913577697 完成训练第 0 折，F1分数: 0.8852
2026-01-04 18:48:26,266 - src.utils.model_loader - INFO - 索引 7892926386913577697 开始训练第 1 折, 共 3 折
2026-01-04 18:48:26,268 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 18:55:23,288 - src.utils.model_loader - INFO - 索引 7892926386913577697 完成训练第 1 折，F1分数: 0.8955
2026-01-04 18:55:23,292 - src.utils.model_loader - INFO - 索引 7892926386913577697 开始训练第 2 折, 共 3 折
2026-01-04 18:55:23,294 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:02:21,543 - src.utils.model_loader - INFO - 索引 7892926386913577697 完成训练第 2 折，F1分数: 0.9101
2026-01-04 19:02:21,547 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: 7892926386913577697, 平均F1分数: 0.8970, 最佳F1分数: 0.9101, 最佳模型下标(折数): 2, 以下是所有模型信息:
2026-01-04 19:02:21,554 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.8852
2026-01-04 19:02:21,560 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.8955
2026-01-04 19:02:21,562 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9101
2026-01-04 19:02:21,567 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: -153316114942080376, 具体参数为: {'learning_rate': 2.4105688465268265e-05, 'weight_decay': 0.08301289930263549, 'dropout_rate': 0.15092594139672555, 'batch_size': 16, 'max_length': 256, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 19:02:21,570 - src.utils.model_loader - INFO - 索引 -153316114942080376 开始训练第 0 折, 共 3 折
2026-01-04 19:02:21,573 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:08:44,287 - src.utils.model_loader - INFO - 索引 -153316114942080376 完成训练第 0 折，F1分数: 0.9118
2026-01-04 19:08:44,288 - src.utils.model_loader - INFO - 索引 -153316114942080376 开始训练第 1 折, 共 3 折
2026-01-04 19:08:44,292 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:15:06,782 - src.utils.model_loader - INFO - 索引 -153316114942080376 完成训练第 1 折，F1分数: 0.9124
2026-01-04 19:15:06,786 - src.utils.model_loader - INFO - 索引 -153316114942080376 开始训练第 2 折, 共 3 折
2026-01-04 19:15:06,789 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:21:28,892 - src.utils.model_loader - INFO - 索引 -153316114942080376 完成训练第 2 折，F1分数: 0.9296
2026-01-04 19:21:28,897 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: -153316114942080376, 平均F1分数: 0.9179, 最佳F1分数: 0.9296, 最佳模型下标(折数): 2, 以下是所有模型信息:
2026-01-04 19:21:28,899 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9118
2026-01-04 19:21:28,902 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9124
2026-01-04 19:21:28,905 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9296
2026-01-04 19:21:28,909 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: -8528463264586789096, 具体参数为: {'learning_rate': 2.8173963228871278e-05, 'weight_decay': 0.09061867679714863, 'dropout_rate': 0.14505207545775656, 'batch_size': 8, 'max_length': 64, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 19:21:28,913 - src.utils.model_loader - INFO - 索引 -8528463264586789096 开始训练第 0 折, 共 3 折
2026-01-04 19:21:28,915 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:24:11,060 - src.utils.model_loader - INFO - 索引 -8528463264586789096 完成训练第 0 折，F1分数: 0.9181
2026-01-04 19:24:11,063 - src.utils.model_loader - INFO - 索引 -8528463264586789096 开始训练第 1 折, 共 3 折
2026-01-04 19:24:11,066 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:26:53,243 - src.utils.model_loader - INFO - 索引 -8528463264586789096 完成训练第 1 折，F1分数: 0.9203
2026-01-04 19:26:53,247 - src.utils.model_loader - INFO - 索引 -8528463264586789096 开始训练第 2 折, 共 3 折
2026-01-04 19:26:53,250 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:29:37,718 - src.utils.model_loader - INFO - 索引 -8528463264586789096 完成训练第 2 折，F1分数: 0.9087
2026-01-04 19:29:37,722 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: -8528463264586789096, 平均F1分数: 0.9157, 最佳F1分数: 0.9203, 最佳模型下标(折数): 1, 以下是所有模型信息:
2026-01-04 19:29:37,724 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9181
2026-01-04 19:29:37,727 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9203
2026-01-04 19:29:37,729 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9087
2026-01-04 19:29:37,733 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: 7225097180343850605, 具体参数为: {'learning_rate': 4.3007010062893054e-05, 'weight_decay': 0.08728295845269778, 'dropout_rate': 0.1205428780314684, 'batch_size': 128, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 19:29:37,737 - src.utils.model_loader - INFO - 索引 7225097180343850605 开始训练第 0 折, 共 3 折
2026-01-04 19:29:37,739 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:32:26,476 - src.utils.model_loader - INFO - 索引 7225097180343850605 完成训练第 0 折，F1分数: 0.8994
2026-01-04 19:32:26,480 - src.utils.model_loader - INFO - 索引 7225097180343850605 开始训练第 1 折, 共 3 折
2026-01-04 19:32:26,482 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:35:12,858 - src.utils.model_loader - INFO - 索引 7225097180343850605 完成训练第 1 折，F1分数: 0.9149
2026-01-04 19:35:12,862 - src.utils.model_loader - INFO - 索引 7225097180343850605 开始训练第 2 折, 共 3 折
2026-01-04 19:35:12,865 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:37:59,178 - src.utils.model_loader - INFO - 索引 7225097180343850605 完成训练第 2 折，F1分数: 0.9381
2026-01-04 19:37:59,181 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: 7225097180343850605, 平均F1分数: 0.9174, 最佳F1分数: 0.9381, 最佳模型下标(折数): 2, 以下是所有模型信息:
2026-01-04 19:37:59,183 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.8994
2026-01-04 19:37:59,184 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9149
2026-01-04 19:37:59,187 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9381
2026-01-04 19:37:59,190 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: 516747693372253568, 具体参数为: {'learning_rate': 1.8934904363646238e-05, 'weight_decay': 0.022144380303898276, 'dropout_rate': 0.15023373008626223, 'batch_size': 32, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 19:37:59,193 - src.utils.model_loader - INFO - 索引 516747693372253568 开始训练第 0 折, 共 3 折
2026-01-04 19:37:59,196 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:41:04,838 - src.utils.model_loader - INFO - 索引 516747693372253568 完成训练第 0 折，F1分数: 0.9540
2026-01-04 19:41:04,841 - src.utils.model_loader - INFO - 索引 516747693372253568 开始训练第 1 折, 共 3 折
2026-01-04 19:41:04,844 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:44:11,254 - src.utils.model_loader - INFO - 索引 516747693372253568 完成训练第 1 折，F1分数: 0.9389
2026-01-04 19:44:11,258 - src.utils.model_loader - INFO - 索引 516747693372253568 开始训练第 2 折, 共 3 折
2026-01-04 19:44:11,261 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:47:16,994 - src.utils.model_loader - INFO - 索引 516747693372253568 完成训练第 2 折，F1分数: 0.9381
2026-01-04 19:47:16,997 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: 516747693372253568, 平均F1分数: 0.9437, 最佳F1分数: 0.9540, 最佳模型下标(折数): 0, 以下是所有模型信息:
2026-01-04 19:47:17,000 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9540
2026-01-04 19:47:17,002 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9389
2026-01-04 19:47:17,005 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9381
2026-01-04 19:47:17,009 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: 3166825125378794643, 具体参数为: {'learning_rate': 1.352415177268585e-05, 'weight_decay': 0.09727053429511048, 'dropout_rate': 0.32572491520081154, 'batch_size': 64, 'max_length': 128, 'num_epochs': 3, 'n_folds': 3, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 19:47:17,012 - src.utils.model_loader - INFO - 索引 3166825125378794643 开始训练第 0 折, 共 3 折
2026-01-04 19:47:17,015 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:50:12,459 - src.utils.model_loader - INFO - 索引 3166825125378794643 完成训练第 0 折，F1分数: 0.9424
2026-01-04 19:50:12,463 - src.utils.model_loader - INFO - 索引 3166825125378794643 开始训练第 1 折, 共 3 折
2026-01-04 19:50:12,466 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:53:08,286 - src.utils.model_loader - INFO - 索引 3166825125378794643 完成训练第 1 折，F1分数: 0.9464
2026-01-04 19:53:08,289 - src.utils.model_loader - INFO - 索引 3166825125378794643 开始训练第 2 折, 共 3 折
2026-01-04 19:53:08,293 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 19:56:03,782 - src.utils.model_loader - INFO - 索引 3166825125378794643 完成训练第 2 折，F1分数: 0.9448
2026-01-04 19:56:03,786 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: 3166825125378794643, 平均F1分数: 0.9445, 最佳F1分数: 0.9464, 最佳模型下标(折数): 1, 以下是所有模型信息:
2026-01-04 19:56:03,787 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9424
2026-01-04 19:56:03,790 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9464
2026-01-04 19:56:03,792 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9448
2026-01-04 19:56:03,796 - src.utils.model_loader - INFO - 获取最佳超参数成功，具体数值为:
2026-01-04 19:56:03,798 - src.utils.model_loader - INFO - 	 learning_rate: 1.6081966473745243e-05
2026-01-04 19:56:03,801 - src.utils.model_loader - INFO - 	 weight_decay: 0.02698200177017849
2026-01-04 19:56:03,803 - src.utils.model_loader - INFO - 	 dropout_rate: 0.3067066217763812
2026-01-04 19:56:03,805 - src.utils.model_loader - INFO - 	 batch_size: 32
2026-01-04 19:56:03,808 - src.utils.model_loader - INFO - 	 max_length: 128
2026-01-04 19:56:03,818 - src.utils.model_loader - INFO - 使用最佳超参数进行最终训练，best_config具体数值为: {'learning_rate': 1.6081966473745243e-05, 'weight_decay': 0.02698200177017849, 'dropout_rate': 0.3067066217763812, 'batch_size': 32, 'max_length': 128, 'num_epochs': 10, 'n_folds': 5, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 19:56:03,820 - src.utils.model_loader - INFO - 开始新一轮的K折交叉验证训练, 索引为: -4981001571275211137, 具体参数为: {'learning_rate': 1.6081966473745243e-05, 'weight_decay': 0.02698200177017849, 'dropout_rate': 0.3067066217763812, 'batch_size': 32, 'max_length': 128, 'num_epochs': 10, 'n_folds': 5, 'max_grad_norm': 1.0, 'bert_model_name': 'google-bert/bert-base-chinese', 'num_classes': 2}
2026-01-04 19:56:03,824 - src.utils.model_loader - INFO - 索引 -4981001571275211137 开始训练第 0 折, 共 5 折
2026-01-04 19:56:03,826 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 20:07:38,398 - src.utils.model_loader - INFO - 索引 -4981001571275211137 完成训练第 0 折，F1分数: 0.9540
2026-01-04 20:07:38,402 - src.utils.model_loader - INFO - 索引 -4981001571275211137 开始训练第 1 折, 共 5 折
2026-01-04 20:07:38,406 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 20:19:15,204 - src.utils.model_loader - INFO - 索引 -4981001571275211137 完成训练第 1 折，F1分数: 0.9685
2026-01-04 20:19:15,208 - src.utils.model_loader - INFO - 索引 -4981001571275211137 开始训练第 2 折, 共 5 折
2026-01-04 20:19:15,211 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 20:30:50,996 - src.utils.model_loader - INFO - 索引 -4981001571275211137 完成训练第 2 折，F1分数: 0.9719
2026-01-04 20:30:51,000 - src.utils.model_loader - INFO - 索引 -4981001571275211137 开始训练第 3 折, 共 5 折
2026-01-04 20:30:51,002 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 20:42:28,074 - src.utils.model_loader - INFO - 索引 -4981001571275211137 完成训练第 3 折，F1分数: 0.9685
2026-01-04 20:42:28,077 - src.utils.model_loader - INFO - 索引 -4981001571275211137 开始训练第 4 折, 共 5 折
2026-01-04 20:42:28,080 - src.utils.model_loader - INFO - 从内存缓存加载模型和tokenizer: google-bert/bert-base-chinese
2026-01-04 20:54:06,983 - src.utils.model_loader - INFO - 索引 -4981001571275211137 完成训练第 4 折，F1分数: 0.9727
2026-01-04 20:54:06,986 - src.utils.model_loader - INFO - 此轮的K折交叉验证训练完成, 索引为: -4981001571275211137, 平均F1分数: 0.9671, 最佳F1分数: 0.9727, 最佳模型下标(折数): 4, 以下是所有模型信息:
2026-01-04 20:54:06,989 - src.utils.model_loader - INFO - 	 模型下标(折数) 0: F1分数 = 0.9540
2026-01-04 20:54:06,990 - src.utils.model_loader - INFO - 	 模型下标(折数) 1: F1分数 = 0.9685
2026-01-04 20:54:06,992 - src.utils.model_loader - INFO - 	 模型下标(折数) 2: F1分数 = 0.9719
2026-01-04 20:54:06,993 - src.utils.model_loader - INFO - 	 模型下标(折数) 3: F1分数 = 0.9685
2026-01-04 20:54:06,996 - src.utils.model_loader - INFO - 	 模型下标(折数) 4: F1分数 = 0.9727
2026-01-04 20:54:08,153 - src.utils.model_loader - INFO - 训练曲线已保存到 outputs/loss_curve_advanced.png 和 outputs/accuracy_curve_advanced.png
2026-01-04 20:54:20,008 - src.utils.model_loader - INFO - 模型保存成功，训练完成！
